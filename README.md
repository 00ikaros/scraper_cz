# ğŸ“œ Bloomberg Law Scraper

An interactive web application for scraping legal transcripts from Bloomberg Law with real-time user control and feedback.

## âœ¨ Features

- ğŸ” **Secure Login** - Credentials stored in environment variables
- ğŸ” **Smart Search** - Interactive court selection with fuzzy matching
- ğŸ“„ **Flexible Scraping** - Choose which transcripts to download
- âš¡ **Real-time Updates** - Live progress tracking via WebSocket
- ğŸ¨ **Beautiful UI** - Modern, responsive control panel
- ğŸ“Š **Job Management** - Track and manage scraping jobs
- ğŸ’¾ **Automatic Downloads** - PDFs saved with organized naming
- ğŸ“ **Comprehensive Logging** - Detailed logs for debugging

## ğŸ—ï¸ Architecture
```
Bloomberg Law Scraper
â”œâ”€â”€ Backend (Python + FastAPI + Playwright)
â”‚   â”œâ”€â”€ REST API for job management
â”‚   â”œâ”€â”€ WebSocket for real-time communication
â”‚   â””â”€â”€ Playwright-based browser automation
â”‚
â”œâ”€â”€ Frontend (Vanilla JS + HTML + CSS)
â”‚   â”œâ”€â”€ Interactive control panel
â”‚   â”œâ”€â”€ Real-time status updates
â”‚   â””â”€â”€ Court & transcript selection UI
â”‚
â””â”€â”€ State Machine
    â”œâ”€â”€ Login â†’ Search â†’ Results â†’ Documents
    â””â”€â”€ Interactive pauses for user input
```

## ğŸ“‹ Prerequisites

- Python 3.10 or higher
- Bloomberg Law account with valid credentials
- Google Chrome or Chromium browser (installed automatically by Playwright)

## ğŸš€ Quick Start

### 1. Clone or Extract the Project
```bash
cd bloomberg-scraper
```

### 2. Install Dependencies
```bash
# Install Python packages
pip install -r requirements.txt

# Install Playwright browsers
playwright install chromium
```

### 3. Configure Credentials
```bash
# Copy the example environment file
cp .env.example .env

# Edit .env and add your Bloomberg Law credentials
nano .env  # or use any text editor
```

Update these values in `.env`:
```bash
BLOOMBERG_USERNAME=your_actual_username
BLOOMBERG_PASSWORD=your_actual_password
```

### 4. Run the Application
```bash
# Quick start (recommended)
python run.py

# Or run directly
python main.py
```

### 5. Open the Control Panel

Open your browser and navigate to:
```
http://localhost:8000
```

## ğŸ“– Usage Guide

### Basic Workflow

1. **Start the Application**
```bash
   python run.py
```

2. **Open Control Panel**
   - Navigate to `http://localhost:8000` in your browser
   - Wait for "Connected" status indicator

3. **Configure Search**
   - **Keywords**: e.g., "transcript"
   - **Court Name**: e.g., "U.S. Bankruptcy Court District of Nevada"
   - **Judge Name**: e.g., "Markell"
   - **Number of Documents**: Leave empty for all on page, or specify a number (1-50)

4. **Start Scraping**
   - Click "ğŸš€ Start Scraping"
   - Watch real-time progress in the Activity Log

5. **Interactive Selection**
   - **Court Selection**: When multiple courts match, select the correct one
   - **Transcript Selection**: Choose which transcript entries to download
     - âœ“ Download Selected
     - â¬‡ï¸ Download All
     - â­ï¸ Skip Document

6. **Monitor Progress**
   - View current state and progress bar
   - Check downloaded files in the Downloads panel
   - Review activity log for detailed information

7. **Access Downloads**
   - Files are saved to `./downloads/` directory
   - Naming format: `{docket_number}_entry_{entry_num}.pdf`

### Configuration Options

Edit `.env` to customize behavior:
```bash
# Browser Settings
HEADLESS_MODE=false          # Set to true to hide browser window
BROWSER_TIMEOUT=60000        # Timeout in milliseconds

# Scraping Mode
SCRAPING_MODE=FULLY_INTERACTIVE
# Options:
# - FULLY_INTERACTIVE: Pause for all selections
# - SEMI_AUTOMATED: Only pause when needed
# - FULLY_AUTOMATED: No pauses (requires exact inputs)

# Paths
DOWNLOADS_DIR=./downloads
LOGS_DIR=./logs
SCREENSHOTS_DIR=./screenshots
```

## ğŸ“ Project Structure
```
bloomberg-scraper/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                 # FastAPI entry point
â”‚   â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚   â”œâ”€â”€ .env                    # Configuration (create from .env.example)
â”‚   â”‚
â”‚   â”œâ”€â”€ scraper/
â”‚   â”‚   â”œâ”€â”€ browser_manager.py           # Playwright browser
â”‚   â”‚   â”œâ”€â”€ bloomberg_scraper.py         # Main orchestrator
â”‚   â”‚   â”œâ”€â”€ state_machine.py             # State management
â”‚   â”‚   â””â”€â”€ page_handlers/
â”‚   â”‚       â”œâ”€â”€ page1_login_search.py    # Login & search
â”‚   â”‚       â”œâ”€â”€ page2_results.py         # Results handling
â”‚   â”‚       â””â”€â”€ page3_docket.py          # Docket & downloads
â”‚   â”‚
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ websocket_handler.py   # WebSocket server
â”‚   â”‚   â””â”€â”€ routes.py              # REST API
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ events.py              # WebSocket events
â”‚   â”‚   â””â”€â”€ scraping_job.py        # Job models
â”‚   â”‚
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ settings.py            # Configuration
â”‚   â”‚   â””â”€â”€ selectors.json         # CSS selectors
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logger.py              # Logging
â”‚       â””â”€â”€ helpers.py             # Utilities
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html                 # Control panel
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ styles.css             # Styling
â”‚   â””â”€â”€ js/
â”‚       â”œâ”€â”€ app.js                 # Main logic
â”‚       â”œâ”€â”€ websocket-client.js    # WebSocket client
â”‚       â””â”€â”€ ui-components.js       # UI rendering
â”‚
â”œâ”€â”€ downloads/                     # Downloaded PDFs
â”œâ”€â”€ logs/                          # Application logs
â”œâ”€â”€ screenshots/                   # Debug screenshots
â”‚
â”œâ”€â”€ run.py                         # Quick start script
â””â”€â”€ README.md                      # This file
```

## ğŸ”§ Advanced Usage

### Customizing Transcript Patterns

Edit `config/selectors.json` to add custom transcript patterns:
```json
{
  "transcript_patterns": [
    {
      "id": "hearing_transcript",
      "pattern": "^Transcript regarding Hearing Held on",
      "enabled": true,
      "description": "Standard hearing transcript"
    },
    {
      "id": "trial_transcript",
      "pattern": "^Transcript of Trial",
      "enabled": true,
      "description": "Trial transcript"
    }
  ]
}
```

### API Endpoints

The application exposes REST API endpoints:

- `GET /` - Frontend control panel
- `GET /api/health` - Health check
- `POST /api/jobs/create` - Create scraping job
- `GET /api/jobs/{job_id}` - Get job status
- `GET /api/jobs/{job_id}/results` - Get job results
- `POST /api/jobs/{job_id}/cancel` - Cancel job
- `GET /api/jobs` - List all jobs
- `WS /ws?client_id={id}` - WebSocket connection

API Documentation: `http://localhost:8000/docs`

### Programmatic Usage
```python
from scraper.bloomberg_scraper import BloombergScraper
from models.scraping_job import ScrapingJob, SearchCriteria
from api.websocket_handler import ConnectionManager

# Create job
search_criteria = SearchCriteria(
    keywords="transcript",
    court_name="U.S. Bankruptcy Court District of Nevada",
    judge_name="Markell"
)

job = ScrapingJob(
    job_id="job_123",
    search_criteria=search_criteria,
    num_documents=5
)

# Run scraper
connection_manager = ConnectionManager()
scraper = BloombergScraper("client_id", connection_manager)
await scraper.run_scraping_job(job)
```

## ğŸ› Troubleshooting

### Common Issues

**1. Login Fails**
- Verify credentials in `.env` file
- Check if Bloomberg Law changed their login page
- Review `logs/scraper_*.log` for details

**2. Court Not Found**
- Try a more specific or broader court name
- Check the interactive selection dialog
- Courts must be typed exactly as they appear in Bloomberg Law

**3. No Transcripts Found**
- Verify search criteria (keywords, court, judge)
- Check if transcripts exist for that case
- Review transcript patterns in `config/selectors.json`

**4. WebSocket Connection Failed**
- Ensure no firewall blocking port 8000
- Check if another application is using the port
- Try restarting the application

**5. Downloads Not Appearing**
- Check `downloads/` directory
- Verify file permissions
- Look for errors in Activity Log

### Debug Mode

Enable debug logging in `.env`:
```bash
LOG_LEVEL=DEBUG
HEADLESS_MODE=false  # See browser actions
```

Check logs:
```bash
# All logs
tail -f logs/scraper_*.log

# Errors only
tail -f logs/errors_*.log
```

Take screenshots on error (automatic):
```bash
ls screenshots/
```

## ğŸ“Š Performance Tips

- **Headless Mode**: Set `HEADLESS_MODE=true` for faster scraping
- **Batch Size**: Process 5-10 documents at a time for optimal speed
- **Network**: Ensure stable internet connection
- **Resources**: Close other browser tabs to free up memory

## ğŸ”’ Security

- âœ… Credentials stored in `.env` (never commit to git)
- âœ… HTTPS for Bloomberg Law connections
- âœ… Session state management
- âœ… No credential logging
- âš ï¸  `.env` is in `.gitignore` - keep it safe!

## ğŸ“œ License

This project is for educational and research purposes. Ensure compliance with Bloomberg Law's Terms of Service when using this tool.

## ğŸ¤ Contributing

Suggestions and improvements welcome! Key areas:
- Additional court systems (PACER integration)
- Enhanced pattern matching
- Export formats (CSV, JSON)
- Scheduling capabilities

## ğŸ“§ Support

For issues or questions:
1. Check the troubleshooting section
2. Review logs in `logs/` directory
3. Check `screenshots/` for visual debugging

## ğŸ¯ Roadmap

- [ ] PACER integration
- [ ] Multi-user support
- [ ] Job scheduling
- [ ] Export to CSV/Excel
- [ ] Email notifications
- [ ] Docker deployment
- [ ] Cloud storage integration

---

**Happy Scraping! ğŸ“œâœ¨**